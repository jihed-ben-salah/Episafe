{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#sequence_length = 50  # Adjust based on your needs\n",
    "num_features = 9\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "\n",
    "# Data loading and preprocessing function\n",
    "def load_and_preprocess_data(file_paths, label_paths,folder):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for j in range(40,len(file_paths[folder])):\n",
    "        #for file_path, label_path in zip(file_paths, label_paths):\n",
    "        df = pd.read_parquet(file_paths[folder][j])\n",
    "        #label_df = pd.read_csv(label_path)\n",
    "        # Assuming binary classification label is in the 'label' column\n",
    "        data.append(df.values)\n",
    "    labels.append(label_paths[folder][40:])\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Generate a list of file and label paths\n",
    "data_dir = 'D:/Orange/Episafe_local/data/parquet/train/1869'\n",
    "file_folders = [folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder).replace(os.path.sep,'/'))]\n",
    "\n",
    "labels_list =[]\n",
    "file_paths = []\n",
    "\n",
    "labels_path = 'D:/Orange/Episafe_local/episafe-data/labels'\n",
    "for folder in file_folders:\n",
    "    folder_path = os.path.join(data_dir, folder).replace(os.path.sep,'/')\n",
    "    folder_files = [os.path.join(folder_path, filename).replace(os.path.sep,'/') for filename in os.listdir(folder_path) if filename.endswith('.parquet')]\n",
    "    #print(len(folder_files))\n",
    "    label_file = [os.path.join(labels_path, file).replace(os.path.sep,'/') for file in os.listdir(labels_path) if file.split('_')[1] == folder]\n",
    "    label_df = pd.read_csv(label_file[0])\n",
    "    file_paths.append(folder_files)\n",
    "    file_labels = []\n",
    "    for i in range(len(os.listdir(folder_path))):\n",
    "        label_value = label_df['label'][i]\n",
    "        file_labels.append(label_value)\n",
    "    labels_list.append(file_labels)\n",
    "        \n",
    "\n",
    "# Build the LSTM model\n",
    "model = tf.keras.Sequential([\n",
    "    LSTM(units=50, input_shape=(76800, 9)),  # None for variable sequence length\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************start of training process*************\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"************start of training process*************\")\n",
    "for epoch in range(num_epochs):\n",
    "    #file_paths, label_paths = shuffle(file_paths, file_labels)  # Shuffle data for each epoch\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    for folder in range(len(file_paths)):\n",
    "        batch_num +=1\n",
    "        batch_x, batch_y = load_and_preprocess_data(file_paths, labels_list,folder)\n",
    "        #print(batch_y[0])\n",
    "        batch_loss = model.train_on_batch(batch_x, batch_y[0])\n",
    "        total_loss += batch_loss  \n",
    "        model.reset_states()  # Reset states at the end of each batch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Batch {batch_num} - Loss: {batch_loss:.4f}\")\n",
    "    average_loss = total_loss / len(file_paths)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}\")\n",
    "    model.reset_states()  # Reset states at the end of each epoch\n",
    "\n",
    "# Save the trained model\n",
    "#model.save('trained_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
